{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Implementation of matrix representation, bag of word model, count vectorizer, tfIdf vectorizer, cosine similarity"
      ],
      "metadata": {
        "id": "EtFItlR6oDhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CountVectorizer"
      ],
      "metadata": {
        "id": "vve1DcMlFbqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk"
      ],
      "metadata": {
        "id": "3J0WFqkKN4gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "a = CountVectorizer(stop_words='english')\n",
        "X = [\"Computers can analyze text\",\n",
        "     \"They do it using vectors and matrices\",\n",
        "     \"Computers can process massive amounts of text data\"]\n",
        "x_vect = a.fit_transform(x)\n",
        "print(x_vect)\n",
        "print(x_vect.todense())\n",
        "print(a.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8J9yXYWFmin",
        "outputId": "659b452c-8edb-4ab6-ee64-2b10ab805e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 6)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 4)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 6)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 3)\t1\n",
            "  (2, 2)\t1\n",
            "[[1 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0 0 1 1]\n",
            " [0 1 1 1 0 1 1 0 0]]\n",
            "{'computer': 1, 'analyze': 0, 'text': 6, 'using': 7, 'vexters': 8, 'matriices': 4, 'processes': 5, 'massive': 3, 'data': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "            \"Natural Language Processing making computers comprehend language data\",\n",
        "            \"The field of Natural Language Processing is evolving everyday\"]\n",
        "\n",
        "sentences = [i.lower() for i in sentences]"
      ],
      "metadata": {
        "id": "dBTwIhf0IQRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer,SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKsx6pucOm02",
        "outputId": "93c9a4f4-987d-49c0-ec2a-448c6080017e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "corpus = pd.Series(sentences).astype(str)"
      ],
      "metadata": {
        "id": "VjK0lreJNuRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(text,keep_list):\n",
        "  cleaned_text = pd.Series()\n",
        "  for row in text:\n",
        "    te = []\n",
        "    for j in row.split():\n",
        "      if j not in keep_list:\n",
        "        p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string = j)\n",
        "        p1 = p1.lower()\n",
        "        te.append(p1)\n",
        "      else:\n",
        "        te.append(j)\n",
        "    cleaned_text = cleaned_text.append(pd.Series(\" \".join(te)))\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "7omnqjutOTsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(text):\n",
        "  wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for i in wh_words:\n",
        "    stop_words.remove(i)\n",
        "\n",
        "  new_text = pd.Series()\n",
        "  te1 = []\n",
        "  for row in text:\n",
        "    te = []\n",
        "    for word in row.split():\n",
        "        if(word not in stop_words):\n",
        "          te.append(word)\n",
        "    new_text = new_text.append(pd.Series(' '.join(te)))\n",
        "    #new_text = new_text.append(pd.Series(\" \".join(te)))\n",
        "\n",
        "  #print(new_text)\n",
        "  return new_text"
      ],
      "metadata": {
        "id": "IoMs3QXZOis0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text, keep_list, cleanning=True, stemi=False, stem_type=None, lemmatiz=False, removal_stopwords = False):\n",
        "\n",
        "  if cleanning == True:\n",
        "    text = text_clean(text,keep_list)\n",
        "  print(\"ho\")\n",
        "  if removal_stopwords==True:\n",
        "    text = stopwords_removal(text)\n",
        "  else:\n",
        "    text = [[x for x in x.split()] for x in text]\n",
        "  print(\"ho\")\n",
        "  if lemmatiz == True:\n",
        "    text = lemmatization(text)\n",
        "  print(\"ho\")\n",
        "  if stemi == True:\n",
        "    text = stemming(text,stem_type)\n",
        "  text = [''.join(i) for i in text]\n",
        "  return text"
      ],
      "metadata": {
        "id": "BOkQjaJiOMtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_list=[]\n",
        "preprocessed_corpus = preprocessing(corpus,keep_list,lemmatiz =False,removal_stopwords=True)\n",
        "print(preprocessed_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-FPZritNu-u",
        "outputId": "fdfcad21-4022-4b3f-e442-8ecde73f951e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ho\n",
            "ho\n",
            "ho\n",
            "['reading natural language processing', 'natural language processing making computers comprehend language data', 'field natural language processing evolving everyday']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-b0bdc0c10fc9>:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_text = pd.Series()\n",
            "<ipython-input-16-77b5a8ae4c58>:7: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  new_text = pd.Series()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "li = []\n",
        "import numpy as np\n",
        "for i in preprocessed_corpus:\n",
        "  for j in i.split():\n",
        "    if j not in li:\n",
        "      li.append(j)\n",
        "li = np.sort(li)\n",
        "print(\"Unique item \",li)\n",
        "iteration = []\n",
        "for i in li:\n",
        "  count = 0\n",
        "  for j in preprocessed_corpus:\n",
        "    for k in j.split():\n",
        "      if i == k:\n",
        "        count = count + 1\n",
        "  iteration.append(count)\n",
        "\n",
        "\n",
        "\n",
        "li = np.array(li)\n",
        "iteration = np.array(iteration)\n",
        "print(\"Unique item with there count\")\n",
        "print(np.concatenate((li.reshape(len(li),1),iteration.reshape(len(iteration),1)),1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oQo2AhSPDCK",
        "outputId": "bfc07d49-c6cb-4b5f-9737-4f3801d7fb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique item  ['comprehend' 'computers' 'data' 'everyday' 'evolving' 'field' 'language'\n",
            " 'making' 'natural' 'processing' 'reading']\n",
            "Unique item with there count\n",
            "[['comprehend' '1']\n",
            " ['computers' '1']\n",
            " ['data' '1']\n",
            " ['everyday' '1']\n",
            " ['evolving' '1']\n",
            " ['field' '1']\n",
            " ['language' '4']\n",
            " ['making' '1']\n",
            " ['natural' '3']\n",
            " ['processing' '3']\n",
            " ['reading' '1']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect = {}\n",
        "\n",
        "for i,j in enumerate(li):\n",
        "  vect[j]=i\n",
        "print(vect)\n",
        "len(vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgigEt-IPG6a",
        "outputId": "68569a1c-b918-40cc-cc5a-fae4de21e8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'comprehend': 0, 'computers': 1, 'data': 2, 'everyday': 3, 'evolving': 4, 'field': 5, 'language': 6, 'making': 7, 'natural': 8, 'processing': 9, 'reading': 10}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha1 = np.zeros((len(sentences),len(vect)))\n",
        "print(alpha1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku7Vf4KrPIti",
        "outputId": "2d66837c-c006-4370-c655-cac2c429ca1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect['data']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uObS6xW_PIwE",
        "outputId": "c6ed5dcf-790f-428f-f119-6e13c68aad95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha3 = []\n",
        "\n",
        "li = list(li)\n",
        "count = 0\n",
        "while(count<len(sentences)):\n",
        "  temp = []\n",
        "  for i in vect:\n",
        "    if i in sentences[count]:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  count = count + 1\n",
        "  alpha3.append(temp)\n",
        "\n",
        "alpha3 = np.array(alpha3)\n",
        "print(alpha3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D807B41DPIyT",
        "outputId": "fdaf58c9-131c-4dae-fdee-eeaa24e349f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 1 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in enumerate(sentences):\n",
        "  for k in j.split():\n",
        "    if k in vect:\n",
        "      l = vect[k]\n",
        "      alpha1[i][l] = 1\n",
        "print(alpha1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXt45Ob3PI2Y",
        "outputId": "b06dd01a-cec6-49a0-bd1b-dad148302f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "alpha2 = vectorizer.fit_transform(preprocessed_corpus)\n",
        "print(alpha2.todense())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_x7r0T5PI4h",
        "outputId": "e49eea13-df2f-41ff-fb38-f13bb89608b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(vectorizer.vocabulary_.items()))\n",
        "print(vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV6_VntCPYlN",
        "outputId": "1e7aee26-2afc-4845-ff42-1293887b3152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('comprehend', 0), ('computers', 1), ('data', 2), ('everyday', 3), ('evolving', 4), ('field', 5), ('language', 6), ('making', 7), ('natural', 8), ('processing', 9), ('reading', 10)]\n",
            "{'comprehend': 0, 'computers': 1, 'data': 2, 'everyday': 3, 'evolving': 4, 'field': 5, 'language': 6, 'making': 7, 'natural': 8, 'processing': 9, 'reading': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Su3MNGGxLIeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6DbwgfkN69i",
        "outputId": "fbdf532c-a9e7-4c8b-9bc2-7d4fd2e2a750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "text = '''\n",
        "We are reading about Natural Language Processing Here,\n",
        "Natural Language Processing making computers comprehend language data\n",
        "The field of Natural Language Processing is evolving everyday\n",
        "'''\n",
        "print(\"\\nOriginal string:\")\n",
        "print(text)\n",
        "clean_word_list = [word for word in text.split() if word not in stoplist]\n",
        "print(\"\\nAfter removing stop words from the said text:\")\n",
        "print(clean_word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hotvHcpLNKe1",
        "outputId": "c1d62cbc-94ba-4fd6-a5a4-a1b7ec13c971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original string:\n",
            "\n",
            "We are reading about Natural Language Processing Here,\n",
            "Natural Language Processing making computers comprehend language data\n",
            "The field of Natural Language Processing is evolving everyday\n",
            "\n",
            "\n",
            "After removing stop words from the said text:\n",
            "['We', 'reading', 'Natural', 'Language', 'Processing', 'Here,', 'Natural', 'Language', 'Processing', 'making', 'computers', 'comprehend', 'language', 'data', 'The', 'field', 'Natural', 'Language', 'Processing', 'evolving', 'everyday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "YEFNmQUKO5eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_corpus = [\"We are reading about Natural Language Processing Here\",\n",
        "            \"Natural Language Processing making computers comprehend language data\",\n",
        "            \"The field of Natural Language Processing is evolving everyday\"]\n",
        "stoplist = set('for a of the and to in on of to are at'.split(' '))\n",
        "processed_text = [[word for word in document.lower().split() if word not in stoplist]for document in txt_corpus]\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF6KucsKPOJ8",
        "outputId": "4780c37d-526b-4390-f4d7-da8c938a72e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['we', 'reading', 'about', 'natural', 'language', 'processing', 'here'], ['natural', 'language', 'processing', 'making', 'computers', 'comprehend', 'language', 'data'], ['field', 'natural', 'language', 'processing', 'is', 'evolving', 'everyday']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FckgRLZiaAFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}